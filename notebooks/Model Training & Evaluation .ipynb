{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks = pd.read_csv('/Users/chenzizhang/Data Science/data/processed/vehicles_2.csv', chunksize=5000)\n",
    "chunk_list = [] \n",
    "for chunk in df_chunks:\n",
    "    chunk_list.append(chunk)\n",
    "\n",
    "df = pd.concat(chunk_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of regression algorithm for model training. Considering the estimation of used car prices is a regression problem, thus I chose $\\color{red}{\\text{Linear Regression}}$, $\\color{red}{\\text{SGD Regression}}$, $\\color{red}{\\text{Decision Tree Regression}}$, $\\color{red}{\\text{LGBM}}$, $\\color{red}{\\text{XGBOOST}}$ algorithms to train the model respectively. Then, I will compare three models using prediction accuracy indicator RMSE.\n",
    "\n",
    "Reference: https://zhuanlan.zhihu.com/p/62034592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'price'\n",
    "Y = df[target]\n",
    "X = df.drop([target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, linear regression is used as a baseline regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(x_train, y_train)\n",
    "Y_pred_reg = reg.predict(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 SGD Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent is a simple yet very efficient approach to fit linear models. It is particularly useful when the number of samples (and the number of features) is very large.\n",
    "The class SGDRegressor implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. $\\color{red}{\\text{SGDRegressor is well suited for regression problems with a large number of training samples (> 10.000)}}$, for other problems we recommend Ridge, Lasso, or ElasticNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "SGD = linear_model.SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "SGD.fit(x_train, y_train)\n",
    "Y_pred_SGD = SGD.predict(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are divided into Classification and Regression Trees. Regression trees are needed when the response variable is numeric or continuous. Classification trees, as the name implies are used to separate the dataset into classes belonging to the response variable. This piece explains a Decision Tree Regression Model practice with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(x_train, y_train)\n",
    "Y_pred_dtr = dtr.predict(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenzizhang/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = lgb.Dataset(x_train, y_train, silent = False)\n",
    "valid_set = lgb.Dataset(x_test, y_test, silent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/vbmokin/used-cars-price-prediction-by-15-models/notebook\n",
    "params = {\n",
    "        'boosting_type':'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': -1,\n",
    "        'subsample': 0.8,\n",
    "        'bagging_fraction' : 1,\n",
    "        'max_bin' : 5000 ,\n",
    "        'bagging_freq': 20,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'metric': 'rmse',\n",
    "        'min_split_gain': 0.5,\n",
    "        'min_child_weight': 1,\n",
    "        'min_child_samples': 10,\n",
    "        'scale_pos_weight':1,\n",
    "        'zero_as_missing': False,\n",
    "        'seed':0,        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 8000 rounds\n",
      "[500]\tvalid_0's rmse: 0.00105308\n",
      "[1000]\tvalid_0's rmse: 0.00105308\n",
      "[1500]\tvalid_0's rmse: 0.00105308\n",
      "[2000]\tvalid_0's rmse: 0.00105308\n",
      "[2500]\tvalid_0's rmse: 0.00105308\n",
      "[3000]\tvalid_0's rmse: 0.00105308\n",
      "[3500]\tvalid_0's rmse: 0.00105308\n",
      "[4000]\tvalid_0's rmse: 0.00105308\n",
      "[4500]\tvalid_0's rmse: 0.00105308\n",
      "[5000]\tvalid_0's rmse: 0.00105308\n",
      "[5500]\tvalid_0's rmse: 0.00105308\n",
      "[6000]\tvalid_0's rmse: 0.00105308\n",
      "[6500]\tvalid_0's rmse: 0.00105308\n",
      "[7000]\tvalid_0's rmse: 0.00105308\n",
      "[7500]\tvalid_0's rmse: 0.00105308\n",
      "[8000]\tvalid_0's rmse: 0.00105308\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's rmse: 0.00105308\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.train(params, train_set = train_set, num_boost_round = 10000,\n",
    "                     early_stopping_rounds = 8000, verbose_eval = 500,\n",
    "                     valid_sets = valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_lgb = lgb_model.predict(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is an ensemble tree method that apply the principle of boosting weak learners (CARTs generally) using the gradient descent architecture. XGBoost improves upon the base Gradient Boosting Machines (GBM) framework through systems optimization and algorithmic enhancements.\n",
    "\n",
    "Reference: https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBRegressor() \n",
    "xgb_clf.fit(x_train, y_train)\n",
    "Y_pred_xgb_clf = xgb_clf.predict(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for xgb: 1.3922660937840372e-05\n",
      "Mean Squared Error for xgb: 1.1130616156115258e-06\n",
      "RMSE for xgb: 0.0010550173532276736\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "mae_xgb_clf = sklearn.metrics.mean_absolute_error(y_test, Y_pred_xgb_clf)\n",
    "mse_xgb_clf = sklearn.metrics.mean_squared_error(y_test, Y_pred_xgb_clf)\n",
    "rmse_xgb_clf = sqrt(mse_xgb_clf)\n",
    "print('Mean Absolute Error for xgb:', mae_xgb_clf)\n",
    "print('Mean Squared Error for xgb:', mse_xgb_clf)\n",
    "print('RMSE for xgb:', rmse_xgb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's do $\\color{red}{\\text{Grid search}}$ towards xgbRegressor as $\\color{red}{\\text{parameter tuning}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 21 candidates, totalling 42 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  42 out of  42 | elapsed:  7.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.326\n",
      "Best parameters set: {'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 7, 'min_child_weight': 4, 'n_estimators': 500, 'nthread': 4, 'objective': 'reg:linear', 'silent': 1, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "xgb_1 = xgb.XGBRegressor()\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:linear'],\n",
    "              'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.09, 0,1], #so called `eta` value\n",
    "              'max_depth': [5, 6, 7],\n",
    "              'min_child_weight': [4],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [500]}\n",
    "xgb_grid = GridSearchCV(xgb_1, parameters, cv = 2, n_jobs = 5, verbose=True).fit(x_train, y_train)\n",
    "print(\"Best score: %0.3f\" % xgb_grid.best_score_)\n",
    "print(\"Best parameters set:\", xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_xgb_grid = xgb_grid.predict(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for xgb: 6.654373872458162e-05\n",
      "Mean Squared Error for xgb: 1.926790330466165e-06\n",
      "RMSE for xgb: 0.0013880887329224185\n"
     ]
    }
   ],
   "source": [
    "mae_xgb_grid = sklearn.metrics.mean_absolute_error(y_test, Y_pred_xgb_grid)\n",
    "mse_xgb_grid = sklearn.metrics.mean_squared_error(y_test, Y_pred_xgb_grid)\n",
    "rmse_xgb_grid = sqrt(mse_xgb_grid)\n",
    "print('Mean Absolute Error for xgb:', mae_xgb_grid)\n",
    "print('Mean Squared Error for xgb:', mse_xgb_grid)\n",
    "print('RMSE for xgb:', rmse_xgb_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the performance of XGBoostRegression Algorithm after grid search (parameter tuning) is worse than basic XGBoostRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I used Mean Absolute Error, Mean Squared Error, Root Mean Square Error to compare the performance of the above five models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for linear regression: 4.222730515245781e-05\n",
      "Mean Squared Error for linear regression: 1.110787566610547e-06\n",
      "RMSE for linear regression: 0.0010539390715836219\n"
     ]
    }
   ],
   "source": [
    "# Linear Regreesion\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "mae_reg = sklearn.metrics.mean_absolute_error(y_test, Y_pred_reg)\n",
    "mse_reg = sklearn.metrics.mean_squared_error(y_test, Y_pred_reg)\n",
    "rmse_reg = sqrt(mse_reg)\n",
    "print('Mean Absolute Error for linear regression:', mae_reg)\n",
    "print('Mean Squared Error for linear regression:', mse_reg)\n",
    "print('RMSE for linear regression:', rmse_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for linear regression: 1.4390628220122832e-05\n",
      "Mean Squared Error for linear regression: 1.1081851737717412e-06\n",
      "RMSE for linear regression: 0.0010527037445415216\n"
     ]
    }
   ],
   "source": [
    "# SGD Regression\n",
    "mae_SGD = sklearn.metrics.mean_absolute_error(y_test, Y_pred_SGD)\n",
    "mse_SGD = sklearn.metrics.mean_squared_error(y_test, Y_pred_SGD)\n",
    "rmse_SGD = sqrt(mse_SGD)\n",
    "print('Mean Absolute Error for SGD regression:', mae_SGD)\n",
    "print('Mean Squared Error for SGD regression:', mse_SGD)\n",
    "print('RMSE for SGD regression:', rmse_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for decision tree regression: 8.933313045823305e-06\n",
      "Mean Squared Error for decision tree regression: 1.1081831788114237e-06\n",
      "RMSE for decision tree regression: 0.0010527027969999053\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regression\n",
    "mae_dtr = sklearn.metrics.mean_absolute_error(y_test, Y_pred_dtr)\n",
    "mse_dtr = sklearn.metrics.mean_squared_error(y_test, Y_pred_dtr)\n",
    "rmse_dtr = sqrt(mse_dtr)\n",
    "print('Mean Absolute Error for decision tree regression:', mae_dtr)\n",
    "print('Mean Squared Error for decision tree regression:', mse_dtr)\n",
    "print('RMSE for decision tree regression:', rmse_dtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for LightGBM: 2.2715692714974698e-05\n",
      "Mean Squared Error for LightGBM: 1.1082376533448567e-06\n",
      "RMSE for LightGBM: 0.0010527286703347909\n"
     ]
    }
   ],
   "source": [
    "# LGBM\n",
    "mae_lgb = sklearn.metrics.mean_absolute_error(y_test, Y_pred_lgb)\n",
    "mse_lgb = sklearn.metrics.mean_squared_error(y_test, Y_pred_lgb)\n",
    "rmse_lgb = sqrt(mse_lgb)\n",
    "print('Mean Absolute Error for LightGBM:', mae_lgb)\n",
    "print('Mean Squared Error for LightGBM:', mse_lgb)\n",
    "print('RMSE for LightGBM:', rmse_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for xgb: 1.3922660937840372e-05\n",
      "Mean Squared Error for xgb: 1.1130616156115258e-06\n",
      "RMSE for xgb: 0.0010550173532276736\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "mae_xgb_clf = sklearn.metrics.mean_absolute_error(y_test, Y_pred_xgb_clf)\n",
    "mse_xgb_clf = sklearn.metrics.mean_squared_error(y_test, Y_pred_xgb_clf)\n",
    "rmse_xgb_clf = sqrt(mse_xgb_clf)\n",
    "print('Mean Absolute Error for xgb:', mae_xgb_clf)\n",
    "print('Mean Squared Error for xgb:', mse_xgb_clf)\n",
    "print('RMSE for xgb:', rmse_xgb_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Absolute Error</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "      <th>Root Mean Squared Error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.000042227</td>\n",
       "      <td>0.000001111</td>\n",
       "      <td>0.001053939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD Regression</th>\n",
       "      <td>0.000014391</td>\n",
       "      <td>0.000001108</td>\n",
       "      <td>0.001052704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Regression</th>\n",
       "      <td>0.000008933</td>\n",
       "      <td>0.000001108</td>\n",
       "      <td>0.001052703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM</th>\n",
       "      <td>0.000022716</td>\n",
       "      <td>0.000001108</td>\n",
       "      <td>0.001052729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.000013923</td>\n",
       "      <td>0.000001113</td>\n",
       "      <td>0.001055017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Mean Absolute Error  Mean Squared Error  \\\n",
       "Algorithm                                                           \n",
       "Linear Regression                 0.000042227         0.000001111   \n",
       "SGD Regression                    0.000014391         0.000001108   \n",
       "Decision Tree Regression          0.000008933         0.000001108   \n",
       "LGBM                              0.000022716         0.000001108   \n",
       "XGBoost                           0.000013923         0.000001113   \n",
       "\n",
       "                          Root Mean Squared Error  \n",
       "Algorithm                                          \n",
       "Linear Regression                     0.001053939  \n",
       "SGD Regression                        0.001052704  \n",
       "Decision Tree Regression              0.001052703  \n",
       "LGBM                                  0.001052729  \n",
       "XGBoost                               0.001055017  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = ['Linear Regression', 'SGD Regression', 'Decision Tree Regression', 'LGBM', 'XGBoost']\n",
    "mae = [mae_reg, mae_SGD, mae_dtr, mae_lgb, mae_xgb_clf]\n",
    "mse = [mse_reg, mse_SGD, mse_dtr, mse_lgb, mse_xgb_clf]\n",
    "rmse = [rmse_reg, rmse_SGD, rmse_dtr, rmse_lgb, rmse_xgb_clf]\n",
    "dict = {'Algorithm': index,\n",
    "       'Mean Absolute Error':mae,\n",
    "       'Mean Squared Error': mse,\n",
    "       'Root Mean Squared Error': rmse}\n",
    "pd.set_option('precision', 9)\n",
    "df = pd.DataFrame(dict)\n",
    "df.set_index('Algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the performance evaluation matrix:\n",
    "\n",
    "(1) For Mean Absolute Error indicator, Decision Tree Regression < XGBoost < SGD Regression < LGBM < Linear Regression.\n",
    "\n",
    "(2) For Mean Squared Error indicator, Decision Tree Regression, SGD Regression, LGBM < Linear Regression < XGBoost.\n",
    "\n",
    "(3) For Root Mean Squared Error indicator, Decision Tree Regression < SGD Regression < LGBM < Linear Regression < XGBoost.\n",
    "\n",
    "Thus, $\\color{red}{\\text{Decision Tree Regression}}$ achieves best performance in used car price prediction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
